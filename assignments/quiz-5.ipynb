{"cells":[{"cell_type":"markdown","metadata":{},"source":["- ASSIGNMENT: quiz-5\n","- POINTS: 3\n","- CATEGORY: quiz\n","- RUBRIC: default\n","- RUBRIC_CATEGORIES: technical, presentation\n","- RUBRIC_WEIGHTS: 0.8, 0.2\n","- DUEDATE: 2018-11-28 10:20:00\n","- GRADER: John Kitchin\n"]},{"cell_type":"markdown","metadata":{},"source":["**This is a quiz. You must be present in class to get credit for it. All your work must be your own, and turning this in means you agree that you worked alone on this assignment.**\n\nNewton's method is an iterative method based on finding roots using information about the derivative. There is an improvement if we use the Hessian shown below:\n\n$x_{n+1} = x_n - \\mathbf{H(x_n)}^{-1} \\mathbf{\\nabla f(x_n)}$\n\nwhere $\\mathbf{H(x_n)}$ is the Hessian matrix, and $\\nabla f(x_n)$ is the gradient of $f$ evaluated at $x_n$, which may be a vector. $f$ is a scalar function. This algorithm is still iterative, and starts from an initial guess.\n\nUse this information with autograd to find a minimum of the rosenbrock function starting at the point (5.0, 5.0). Verify you have found a minimum.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def rosenbrock(X):\n    x, y = X\n    return (1 - x)**2 + 100 * (y - x**2)**2"]}],"metadata":{"org":{"ASSIGNMENT":"quiz-5","POINTS":"3","CATEGORY":"quiz","RUBRIC":"default","RUBRIC_CATEGORIES":"technical, presentation","RUBRIC_WEIGHTS":"0.8, 0.2","DUEDATE":"2018-11-28 10:20:00","GRADER":"John Kitchin"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}
