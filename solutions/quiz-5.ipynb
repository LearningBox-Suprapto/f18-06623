{"cells":[{"cell_type":"markdown","metadata":{},"source":["\n# ASSIGNED Using the Hessian in optimization\n\n"]},{"cell_type":"markdown","metadata":{},"source":["**This is a quiz. You must be present in class to get credit for it. All your work must be your own, and turning this in means you agree that you worked alone on this assignment.**\n\nNewton's method is an iterative method based on finding roots using information about the derivative. There is an improvement if we use the Hessian shown below:\n\n$x_{n+1} = x_n - \\mathbf{H(x_n)}^{-1} \\mathbf{\\nabla f(x_n)}$\n\nwhere $\\mathbf{H(x_n)}$ is the Hessian matrix, and $\\nabla f(x_n)$ is the gradient of $f$ evaluated at $x_n$, which may be a vector. $f$ is a scalar function. This algorithm is still iterative, and starts from an initial guess.\n\nUse this information with autograd to find a minimum of the rosenbrock function starting at the point (5.0, 5.0). Verify you have found a minimum.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def rosenbrock(X):\n    x, y = X\n    return (1 - x)**2 + 100 * (y - x**2)**2"]},{"cell_type":"markdown","metadata":{},"source":["\n## solution\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The solution to this problem is to use autograd to get functions fro the gradient and hessian, and then implement the formula provided inside a loop. Here I do that, and print the intermediate steps. You can see by the third step the output is almost constant, and by the 5th step it is constant, indicating no further changes are occurring.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[  4.99900025  24.9900025 ]\n[  1.00079924 -14.98401219]\n[ 1.00079899  1.00159862]\n[ 1.          0.99999936]\n[ 1.  1.]\n[ 1.  1.]\n\narray([ 1.,  1.])"}],"source":["x = np.array([5.0, 5.0])\n\ndef rosenbrock(X):\n    x, y = X\n    return (1 - x)**2 + 100 * (y - x**2)**2\n\nfrom autograd import grad, hessian\ndf = grad(rosenbrock)\ndH = hessian(rosenbrock)\n\nfor i in range(6):\n    x = x - np.linalg.inv(dH(x)) @ df(x)\n    print(x)\nx"]},{"cell_type":"markdown","metadata":{},"source":["Based on the output above, it appears the minimum is at $\\mathbf{X}=(1, 1)$. This is the same as we saw before in lecture. We can confirm it is a stationary point be looking at the gradients.  We can see the first derivatives are zero, which means we have found a stationary point.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[ 0.  0.]"}],"source":["print(df(x))"]},{"cell_type":"markdown","metadata":{},"source":["Finally to confirm it is a minimum we have to show the Hessian is positive definite, by making sure the eigenvalues are all positive. Here we can see the eigenvalues of the Hessian are all positive, so we also know this is a minimum.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[  1.00160064e+03   3.99360767e-01]"}],"source":["print(np.linalg.eigvals(dH(x)))"]}],"metadata":{"org":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}
